{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4+iOIy8i8SLxyrzoXjfc6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiorolnic/anonim/blob/main/notebookColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCp3NJmRf248"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KDV7jGsmf-du",
        "outputId": "78a10307-9422-4867-ae4a-848e6cc1ed8f"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vhMxxBExgN2B"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-Wv5B0vYgbYU"
      },
      "source": [
        "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k2D3pcc_geR-"
      },
      "source": [
        "def preprocess(sent):\n",
        "    sent = nltk.word_tokenize(sent)\n",
        "    sent = nltk.pos_tag(sent)\n",
        "    return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5m2Qm7nAGtsb",
        "outputId": "2a95df0e-95ff-4bb0-a628-a91863075bc9"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ovmwsGSHgh5z",
        "outputId": "b9fbfc9c-bdb0-4b11-9a8b-693354734e2f"
      },
      "source": [
        "sent = preprocess(ex)\n",
        "sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('European', 'JJ'),\n",
              " ('authorities', 'NNS'),\n",
              " ('fined', 'VBD'),\n",
              " ('Google', 'NNP'),\n",
              " ('a', 'DT'),\n",
              " ('record', 'NN'),\n",
              " ('$', '$'),\n",
              " ('5.1', 'CD'),\n",
              " ('billion', 'CD'),\n",
              " ('on', 'IN'),\n",
              " ('Wednesday', 'NNP'),\n",
              " ('for', 'IN'),\n",
              " ('abusing', 'VBG'),\n",
              " ('its', 'PRP$'),\n",
              " ('power', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('mobile', 'JJ'),\n",
              " ('phone', 'NN'),\n",
              " ('market', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('ordered', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('company', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('alter', 'VB'),\n",
              " ('its', 'PRP$'),\n",
              " ('practices', 'NNS')]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F2TLCmoMqpMk",
        "outputId": "39a647e8-a8d4-44b1-bb23-3d05952af8a9"
      },
      "source": [
        "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "cp = nltk.RegexpParser(pattern)\n",
        "cs = cp.parse(sent)\n",
        "print(cs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  European/JJ\n",
            "  authorities/NNS\n",
            "  fined/VBD\n",
            "  Google/NNP\n",
            "  (NP a/DT record/NN)\n",
            "  $/$\n",
            "  5.1/CD\n",
            "  billion/CD\n",
            "  on/IN\n",
            "  Wednesday/NNP\n",
            "  for/IN\n",
            "  abusing/VBG\n",
            "  its/PRP$\n",
            "  (NP power/NN)\n",
            "  in/IN\n",
            "  (NP the/DT mobile/JJ phone/NN)\n",
            "  (NP market/NN)\n",
            "  and/CC\n",
            "  ordered/VBD\n",
            "  (NP the/DT company/NN)\n",
            "  to/TO\n",
            "  alter/VB\n",
            "  its/PRP$\n",
            "  practices/NNS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9LFkpDxbqzk4",
        "outputId": "c5d7c36c-5207-4634-cfa1-254b45e911e6"
      },
      "source": [
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from pprint import pprint\n",
        "iob_tagged = tree2conlltags(cs)\n",
        "pprint(iob_tagged)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('European', 'JJ', 'O'),\n",
            " ('authorities', 'NNS', 'O'),\n",
            " ('fined', 'VBD', 'O'),\n",
            " ('Google', 'NNP', 'O'),\n",
            " ('a', 'DT', 'B-NP'),\n",
            " ('record', 'NN', 'I-NP'),\n",
            " ('$', '$', 'O'),\n",
            " ('5.1', 'CD', 'O'),\n",
            " ('billion', 'CD', 'O'),\n",
            " ('on', 'IN', 'O'),\n",
            " ('Wednesday', 'NNP', 'O'),\n",
            " ('for', 'IN', 'O'),\n",
            " ('abusing', 'VBG', 'O'),\n",
            " ('its', 'PRP$', 'O'),\n",
            " ('power', 'NN', 'B-NP'),\n",
            " ('in', 'IN', 'O'),\n",
            " ('the', 'DT', 'B-NP'),\n",
            " ('mobile', 'JJ', 'I-NP'),\n",
            " ('phone', 'NN', 'I-NP'),\n",
            " ('market', 'NN', 'B-NP'),\n",
            " ('and', 'CC', 'O'),\n",
            " ('ordered', 'VBD', 'O'),\n",
            " ('the', 'DT', 'B-NP'),\n",
            " ('company', 'NN', 'I-NP'),\n",
            " ('to', 'TO', 'O'),\n",
            " ('alter', 'VB', 'O'),\n",
            " ('its', 'PRP$', 'O'),\n",
            " ('practices', 'NNS', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "apFCJndErI_i",
        "outputId": "ee7f6c69-0554-4d2a-d2e9-f3b0f1662755"
      },
      "source": [
        "ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(ex)))\n",
        "print(ne_tree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (GPE European/JJ)\n",
            "  authorities/NNS\n",
            "  fined/VBD\n",
            "  (PERSON Google/NNP)\n",
            "  a/DT\n",
            "  record/NN\n",
            "  $/$\n",
            "  5.1/CD\n",
            "  billion/CD\n",
            "  on/IN\n",
            "  Wednesday/NNP\n",
            "  for/IN\n",
            "  abusing/VBG\n",
            "  its/PRP$\n",
            "  power/NN\n",
            "  in/IN\n",
            "  the/DT\n",
            "  mobile/JJ\n",
            "  phone/NN\n",
            "  market/NN\n",
            "  and/CC\n",
            "  ordered/VBD\n",
            "  the/DT\n",
            "  company/NN\n",
            "  to/TO\n",
            "  alter/VB\n",
            "  its/PRP$\n",
            "  practices/NNS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOIEU0QYsH5p"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWAlRqzhsO1X"
      },
      "source": [
        "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
        "pprint([(X.text, X.label_) for X in doc.ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VTiCquHsZbJ"
      },
      "source": [
        "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixyy7uC3GOHg"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "def url_to_string(url):\n",
        "    res = requests.get(url)\n",
        "    html = res.text\n",
        "    soup = BeautifulSoup(html, 'html5lib')\n",
        "    for script in soup([\"script\", \"style\", 'aside']):\n",
        "        script.extract()\n",
        "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
        "ny_bb = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')\n",
        "article = nlp(ny_bb)\n",
        "len(article.ents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aiFtt7TG9TB"
      },
      "source": [
        "article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGvY9xizHGGc"
      },
      "source": [
        "labels = [x.label_ for x in article.ents]\n",
        "Counter(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfgoOfzQHNwu"
      },
      "source": [
        "items = [x.text for x in article.ents]\n",
        "Counter(items).most_common(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biIC61zKHWoE"
      },
      "source": [
        "sentences = [x for x in article.sents]\n",
        "print(sentences[20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCcDUThQJXX6"
      },
      "source": [
        "displacy.render(nlp(str(sentences[20])), jupyter=True, style='ent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guTAbyAdJl25"
      },
      "source": [
        "displacy.render(nlp(str(sentences[20])), style='dep', jupyter = True, options = {'distance': 120})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "3u6qgKiZBjc-",
        "outputId": "2158bf78-aa7e-41fd-99b8-21f790c9c2df"
      },
      "source": [
        "[(x.orth_,x.pos_, x.lemma_) for x in [y \n",
        "                                      for y\n",
        "                                      in nlp(str(sentences[20])) \n",
        "                                      if not y.is_stop and y.pos_ != 'PUNCT']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6a0e56e7a0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m [(x.orth_,x.pos_, x.lemma_) for x in [y \n\u001b[1;32m      2\u001b[0m                                       \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                       \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                                       if not y.is_stop and y.pos_ != 'PUNCT']]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUm81QZfBqcH"
      },
      "source": [
        "dict([(str(x), x.label_) for x in nlp(str(sentences[20])).ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "zJUuBrxRlntA",
        "outputId": "76fed218-3ea9-4e78-e7b1-33b53ef193eb"
      },
      "source": [
        "# import\n",
        "import nltk\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "\n",
        "# text to processing\n",
        "sentence = \"Sharon flew to Miami last friday\"\n",
        "\n",
        "# jar of the model\n",
        "jar = './stanford-ner-tagger/stanford-ner.jar'\n",
        "model = './stanford-ner-tagger/ner-model-english.ser.gz'\n",
        "\n",
        "# preparing ner tagger object with choosen jar\n",
        "ner_tagger = StanfordNERTagger(model, jar, encoding='utf8')\n",
        "\n",
        "# tokenization: splitting text\n",
        "words = nltk.word_tokenize(sentence)\n",
        "\n",
        "# performing ner tagger on words\n",
        "print(ner_tagger.tag(words))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
            "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
            "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
            "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b33859a3777d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# preparing ner tagger object with choosen jar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mner_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# tokenization: splitting text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 verbose=verbose)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         self._stanford_model = find_file(model_filename,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    719\u001b[0m         searchpath=(), url=None, verbose=False, is_regex=False):\n\u001b[1;32m    720\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 721\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at ./stanford-ner-tagger/stanford-ner.jar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzyGM4i6w-f8"
      },
      "source": [
        "# from spacy.matcher import Matcher\n",
        "#>>> matcher = Matcher(nlp.vocab)\n",
        "#><>> def extract_full_name(nlp_doc):\n",
        "#...     pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "#...     matcher.add('FULL_NAME', None, pattern)\n",
        "#...     matches = matcher(nlp_doc)\n",
        "#...     for match_id, start, end in matches:\n",
        "#...         span = nlp_doc[start:end]\n",
        "#...         return span.text\n",
        "#...\n",
        "#>>> extract_full_name(about_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp8zjqUDJz0O"
      },
      "source": [
        "add pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2huEKUeJ1sp"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "doc = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sD5OFPMLvR7"
      },
      "source": [
        "overwrite_ents=True "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-FKAHoeLCta"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "doc = nlp(\"MyCorp Inc. is a company in the U.S.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz4IXETHL3T_"
      },
      "source": [
        "ruler = nlp.add_pipe(\"entity_ruler\", config={\"validate\": True})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jID3ig1P3DH"
      },
      "source": [
        "add ent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrL53DDyP2FS"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = English()\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def add_event_ent(matcher, doc, i, matches):\n",
        "    # Get the current match and create tuple of entity label, start and end.\n",
        "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
        "    match_id, start, end = matches[i]\n",
        "    entity = Span(doc, start, end, label=\"EVENT\")\n",
        "    doc.ents += (entity,)\n",
        "    print(entity.text)\n",
        "\n",
        "pattern = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
        "matcher.add(\"GoogleIO\", [pattern], on_match=add_event_ent)\n",
        "doc = nlp(\"This is a text about Google I/O\")\n",
        "matches = matcher(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM8cEs2YQXxC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IaNKtXnQYm3"
      },
      "source": [
        "Add ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcLxUli_QhR-"
      },
      "source": [
        "The EntityRuler can also accept an id attribute for each pattern. Using the id attribute allows multiple patterns to be associated with the same entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "NkjYBdSgQWZD",
        "outputId": "27c9fcd2-51a1-46eb-8e0b-ff21ff06a182"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "doc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
        "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n",
        "\n",
        "doc2 = nlp(\"Apple is opening its first big office in San Fran.\")\n",
        "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-919efadffc3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mruler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entity_ruler\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n\u001b[1;32m      6\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"GPE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pattern\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"LOWER\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"san\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"LOWER\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"francisco\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"san-francisco\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE004\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_component_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E003] Not a valid pipeline component. Expected callable, but got 'entity_ruler' (name: 'None').[E004] If you meant to add a built-in component, use `create_pipe`: `nlp.add_pipe(nlp.create_pipe('entity_ruler'))`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV1vO6hQRQ3L"
      },
      "source": [
        "ruler.to_disk(\"./patterns.jsonl\")\n",
        "new_ruler = nlp.add_pipe(\"entity_ruler\").from_disk(\"./patterns.jsonl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hPYLsh9R2On"
      },
      "source": [
        "from spacy.language import Language\n",
        "from spacy.tokens import Span\n",
        "\n",
        "@Language.component(\"expand_person_entities\")\n",
        "def expand_person_entities(doc):\n",
        "    new_ents = []\n",
        "    for ent in doc.ents:\n",
        "        # Only check for title if it's a person and not the first token\n",
        "        if ent.label_ == \"PERSON\" and ent.start != 0:\n",
        "            prev_token = doc[ent.start - 1]\n",
        "            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
        "                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
        "                new_ents.append(new_ent)\n",
        "            else:\n",
        "                new_ents.append(ent)\n",
        "        else:\n",
        "            new_ents.append(ent)\n",
        "    doc.ents = new_ents\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULazXxyuhNhN"
      },
      "source": [
        "disable everythink except NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axeexe54hMoM"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xanqiHFBhkPT"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oQtLiP_lqoD"
      },
      "source": [
        "from spacy.pipeline.ner import DEFAULT_NER_MODEL\n",
        "config = {\n",
        "   \"moves\": None,\n",
        "   \"update_with_oracle_cut_size\": 100,\n",
        "   \"model\": DEFAULT_NER_MODEL,\n",
        "   \"incorrect_spans_key\": \"incorrect_spans\",\n",
        "}\n",
        "nlp.add_pipe(\"ner\", config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcl6LI0Bmego"
      },
      "source": [
        "\n",
        "doc = nlp(\"This is a sentence.\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "# This usually happens under the hood\n",
        "processed = ner(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6-J5pijmnSD"
      },
      "source": [
        "ner = nlp.add_pipe(\"ner\")\n",
        "for doc in ner.pipe(docs, batch_size=50):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}