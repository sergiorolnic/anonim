{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP92ePY7hhqv/Zq8CCL9dZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiorolnic/anonim/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N0UIqbzDRBAR"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF5QG99HRUou",
        "outputId": "b3116cef-4ac0-4115-b775-5f5d76323558"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets"
      ],
      "metadata": {
        "id": "kVff7CKtRXD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_json(\"https://github.com/sergiorolnic/anonim/blob/main/data/esempio.json?raw=true\")\n",
        "cn = pd.read_csv('https://raw.githubusercontent.com/nicolomantini/lista_cognomi_italiani/master/cognomi.txt', header=None)\n",
        "cn.columns = [\"cognome\"]\n",
        "femal_name = pd.read_json('https://github.com/sergiorolnic/anonim/blob/main/data/ITGivenFemale.json?raw=true')\n",
        "male_name = pd.read_json('https://github.com/sergiorolnic/anonim/blob/main/data/ITGivenMale.json?raw=true')\n",
        "mixt = pd.concat([male_name,femal_name])\n",
        "index = pd.Index(range(1, 9087, 1))\n",
        "mixt.set_index(index)\n",
        "\n",
        "def fill_missing(column_val):\n",
        "    if column_val == \"\": \n",
        "        column_val = mixt[\"name\"].iloc[np.random.randint(9086)]\n",
        "    else:\n",
        "         column_val = column_val\n",
        "    return column_val\n",
        "\n",
        "cn[\"nome\"] = \"\"\n",
        "cn['nome'] = cn['nome'].apply(fill_missing) \n",
        "cn[\"nome\"] = cn[\"nome\"].str.upper()\n",
        "cn[\"cognome\"] = cn[\"cognome\"].str.upper()\n",
        "dataframe = pd.DataFrame(df1)\n",
        "dataframe[\"cognome\"] = cn[\"cognome\"][0:15000]\n",
        "dataframe[\"nome\"] = cn[\"nome\"][0:15000]\n",
        "dataframe2 = pd.DataFrame(df1[0:6740])\n",
        "dataframe2[\"cognome\"] = list(cn[\"cognome\"][15000:21740])\n",
        "dataframe2[\"nome\"] = list(cn[\"nome\"][15000:21740])\n",
        "\n",
        "dataframe =dataframe.append(df1, ignore_index = True)\n",
        "dataframe =dataframe.append(dataframe2, ignore_index = True)\n",
        "dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
        "test_data_df =dataframe[35000:36000]\n",
        "dataframe = dataframe[0:35000]"
      ],
      "metadata": {
        "id": "0cCfmLF6XMO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe[\"full_name\"] = dataframe[\"cognome\"] + \" \" +dataframe[\"nome\"]\n",
        "dataframe.drop(columns=[\"cognome\", \"nome\"], inplace=True)"
      ],
      "metadata": {
        "id": "tkZxMMroazDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"https://github.com/sergiorolnic/anonim/blob/main/data/popolamento_ina_20211112_header.csv.json?raw=true\")"
      ],
      "metadata": {
        "id": "BC1EElQ3RuKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe2 = df.sample(frac=1).reset_index(drop=True)\n",
        "dataframe2[\"full_name\"] = dataframe2[\"cognome\"] + \" \" +dataframe2[\"nome\"]\n",
        "dataframe2.drop(columns=[\"cognome\", \"nome\"], inplace=True)"
      ],
      "metadata": {
        "id": "jl43x-SnSIfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataframe = dataframe2[56000:57000]\n",
        "dataframe2= dataframe2[0:56000]"
      ],
      "metadata": {
        "id": "E33xy3sASpwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe\n"
      ],
      "metadata": {
        "id": "TqizlrWUUa7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols1 = dataframe.columns.tolist()\n",
        "cols2 = dataframe2.columns.tolist()"
      ],
      "metadata": {
        "id": "Z8TTCdaiSYOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "K05nMwncTyaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols1)\n",
        "random.shuffle(cols2)"
      ],
      "metadata": {
        "id": "9OzD4Uz0SpWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_shuffle_data = dataframe[cols1][0:5000]\n",
        "first_shuffle_data2 = dataframe2[cols2][0:10000]"
      ],
      "metadata": {
        "id": "F-ZjcJVUUVM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols1)\n",
        "random.shuffle(cols2)\n",
        "second_shuffle_data = dataframe[cols1][5000:10000]\n",
        "second_shuffle_data2 = dataframe[cols2][10000:20000]"
      ],
      "metadata": {
        "id": "F2EcNdtaVDww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols1)\n",
        "random.shuffle(cols2)\n",
        "third_shuffle_data = dataframe[cols1][10000:15000]\n",
        "third_shuffle_data2 = dataframe[cols2][20000:30000]"
      ],
      "metadata": {
        "id": "NcLpDP6RgKZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols1)\n",
        "random.shuffle(cols2)\n",
        "fourth_shuffle_data = dataframe[cols1][15000:20000]\n",
        "fourth_shuffle_data2 = dataframe[cols2][30000:40000]"
      ],
      "metadata": {
        "id": "ISoRTqrLgVxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols1)\n",
        "random.shuffle(cols2)\n",
        "five_shuffle_data = dataframe[cols1][20000:25000]\n",
        "five_shuffle_data2 = dataframe[cols2][40000:50000]"
      ],
      "metadata": {
        "id": "wtHTRCJCcgxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols1)\n",
        "random.shuffle(cols2)\n",
        "six_shuffle_data = dataframe[cols1][25000:35000]\n",
        "six_shuffle_data2 = dataframe[cols2][50000:55000]"
      ],
      "metadata": {
        "id": "HkVJOuiAcrn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_token_and_labels(dataset, token_array, tag_array):\n",
        "  data = dataset\n",
        "\n",
        " \n",
        "  for index, row in data.iterrows():\n",
        "    tokens = []\n",
        "    tags = []\n",
        "\n",
        "    for x, value in row.iteritems():\n",
        "    \n",
        "  \n",
        "      if x == \"indirizzo\":\n",
        "        y = value.split()\n",
        "        first= True;\n",
        "        for word in y:\n",
        "          tokens.append(str(word))\n",
        "          if first:\n",
        "            tags.append(\"B-Ind\")\n",
        "            first = False\n",
        "          else:\n",
        "            tags.append(\"I-Ind\") \n",
        "      elif x == \"full_name\":\n",
        "        y = value.split()\n",
        "        first= True;\n",
        "        for word in y:\n",
        "          tokens.append(str(word))\n",
        "          if first:\n",
        "            tags.append(\"B-Per\")\n",
        "            first = False\n",
        "          else:\n",
        "            tags.append(\"I-Per\")     \n",
        "\n",
        "      else:\n",
        "        tokens.append(str(value))\n",
        "        tags.append(\"O\") \n",
        "  \n",
        "    token_array.append(tokens)\n",
        "    tag_array.append(tags)"
      ],
      "metadata": {
        "id": "7Ou9IF7AVQs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_docs = []\n",
        "tag_docs = []\n",
        "create_token_and_labels(dataframe,token_docs,tag_docs)\n",
        "create_token_and_labels(first_shuffle_data,token_docs,tag_docs)\n",
        "create_token_and_labels(second_shuffle_data,token_docs,tag_docs)\n",
        "create_token_and_labels(third_shuffle_data,token_docs,tag_docs)\n",
        "create_token_and_labels(fourth_shuffle_data,token_docs,tag_docs)\n",
        "create_token_and_labels(five_shuffle_data,token_docs,tag_docs)\n",
        "create_token_and_labels(six_shuffle_data,token_docs,tag_docs)\n",
        "\n",
        "create_token_and_labels(dataframe2,token_docs,tag_docs)\n",
        "create_token_and_labels(first_shuffle_data2,token_docs,tag_docs)\n",
        "create_token_and_labels(second_shuffle_data2,token_docs,tag_docs)\n",
        "create_token_and_labels(third_shuffle_data2,token_docs,tag_docs)\n",
        "create_token_and_labels(fourth_shuffle_data2,token_docs,tag_docs)\n",
        "create_token_and_labels(five_shuffle_data2,token_docs,tag_docs)\n",
        "create_token_and_labels(six_shuffle_data2,token_docs,tag_docs)\n",
        "\n"
      ],
      "metadata": {
        "id": "za1VpNRcVWg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(token_docs)"
      ],
      "metadata": {
        "id": "6peeaGgmjJo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, tags = token_docs, tag_docs\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
      ],
      "metadata": {
        "id": "0r3b2kvvVr0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "id2tag"
      ],
      "metadata": {
        "id": "Wz2AUvTGVwsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('dbmdz/bert-base-italian-cased')\n",
        "\n",
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, padding=\"max_length\",return_offsets_mapping=True)\n"
      ],
      "metadata": {
        "id": "kbaDR_H4V94L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_encodings = tokenizer(val_texts, is_split_into_words=True,padding=\"max_length\", return_offsets_mapping=True)\n"
      ],
      "metadata": {
        "id": "03uqhy_KlrzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_encodings = tokenizer(test_token, is_split_into_words=True,padding=\"max_length\", return_offsets_mapping=True)"
      ],
      "metadata": {
        "id": "_k6z9IqwltXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        mask = (arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)\n",
        "        doc_enc_labels[mask] = doc_labels[:np.sum(mask)]\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "        \n",
        "      #  doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "       # encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "metadata": {
        "id": "JKkNgP1ZWEyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = encode_tags(test_tag, test_encodings)"
      ],
      "metadata": {
        "id": "aC3RJHtmWHYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ybbToaBgWKKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n"
      ],
      "metadata": {
        "id": "V9CqvMktWMv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n",
        "\n"
      ],
      "metadata": {
        "id": "2OZao1woj5yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings.pop(\"offset_mapping\")\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))"
      ],
      "metadata": {
        "id": "Utaawtygj7XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForTokenClassification"
      ],
      "metadata": {
        "id": "ewYg9LV0WRdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForTokenClassification.from_pretrained('dbmdz/bert-base-italian-cased',num_labels=len(unique_tags))\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss,metrics=metric) # can also use any keras loss fn\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lWhPUYYYWUHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset.shuffle(1000).batch(8), epochs=1,validation_data=val_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "Zfc3nqCCWbqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}