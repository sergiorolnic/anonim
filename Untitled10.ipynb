{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfkkTzzugPmHZPutjRmT1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiorolnic/anonim/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knEvxToOCVO7"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seqeval\n",
        "pip install transformers datasets"
      ],
      "metadata": {
        "id": "jMMlciF_CYlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"https://github.com/sergiorolnic/anonim/blob/main/data/popolamento_ina_20211112_header.csv.json?raw=true\")\n",
        "dataframe2 = df.sample(frac=1).reset_index(drop=True)\n",
        "dataframe2[\"full_name\"] = dataframe2[\"cognome\"] + \" \" +dataframe2[\"nome\"]\n",
        "dataframe2[\"cap\"] = \"\""
      ],
      "metadata": {
        "id": "8AST9b7fCeXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "cols2 = dataframe2.columns.tolist()"
      ],
      "metadata": {
        "id": "xoVj2ymWCn_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols2)\n",
        "first_shuffle_data = dataframe[cols1][0:10000]"
      ],
      "metadata": {
        "id": "-tajVQ2UCvGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols2)\n",
        "second_shuffle_data = dataframe[cols1][10000:20000]"
      ],
      "metadata": {
        "id": "-cuGzQovC2QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols2)\n",
        "third_shuffle_data = dataframe[cols1][20000:30000]"
      ],
      "metadata": {
        "id": "_nN02sy2C4xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(cols2)\n",
        "fourth_shuffle_data = dataframe[cols1][30000:40000]"
      ],
      "metadata": {
        "id": "Ir321wzdC7rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vdvy5jyLDAuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_token_and_labels(dataset, token_array, tag_array):\n",
        "  data = dataset\n",
        "\n",
        " \n",
        "  for index, row in data.iterrows():\n",
        "    tokens = []\n",
        "    tags = []\n",
        "\n",
        "    for x, value in row.iteritems():\n",
        "    \n",
        "  \n",
        "      if x == \"indirizzo\":\n",
        "        y = value.split()\n",
        "        first= True;\n",
        "        for word in y:\n",
        "          tokens.append(str(word))\n",
        "          if first:\n",
        "            tags.append(\"B-Ind\")\n",
        "            first = False\n",
        "          else:\n",
        "            tags.append(\"I-Ind\") \n",
        "      elif x == \"full_name\":\n",
        "        y = value.split()\n",
        "        first= True;\n",
        "        for word in y:\n",
        "          tokens.append(str(word))\n",
        "          if first:\n",
        "            tags.append(\"B-Per\")\n",
        "            first = False\n",
        "          else:\n",
        "            if row[\"sesso\"] == \"M\":\n",
        "              tags.append(\"I-Per-M\")\n",
        "            else:\n",
        "              tags.append(\"I-Per-F\")\n",
        "\n",
        "\n",
        "               \n",
        "\n",
        "      else:\n",
        "        tokens.append(str(value))\n",
        "        tags.append(\"O\") \n",
        "  \n",
        "    token_array.append(tokens)\n",
        "    tag_array.append(tags)"
      ],
      "metadata": {
        "id": "7Ou9IF7AVQs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_docs = []\n",
        "tag_docs = []\n",
        "create_token_and_labels(dataframe,token_docs,tag_docs)\n",
        "create_token_and_labels(first_shuffle_data,token_docs,tag_docs)\n",
        "create_token_and_labels(second_shuffle_data,token_docs,tag_docs)\n",
        "create_token_and_labels(third_shuffle_data,token_docs,tag_docs)\n",
        "create_token_and_labels(fourth_shuffle_data,token_docs,tag_docs)"
      ],
      "metadata": {
        "id": "heZkeRLfDTTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, tags = token_docs, tag_docs\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
      ],
      "metadata": {
        "id": "bYt_KIHSDWEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "id2tag"
      ],
      "metadata": {
        "id": "q4-O-2FMDY4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('dbmdz/bert-base-italian-cased')"
      ],
      "metadata": {
        "id": "jEY84tOHDbyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, padding=\"max_length\",return_offsets_mapping=True)"
      ],
      "metadata": {
        "id": "LG6gwdJTDe_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_encodings = tokenizer(val_texts, is_split_into_words=True,padding=\"max_length\", return_offsets_mapping=True)"
      ],
      "metadata": {
        "id": "KpuqWNZ_Dg81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I3-mZbqDDjY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        mask = (arr_offset[:, 0] == 0) & (arr_offset[:, 1] != 0)\n",
        "        doc_enc_labels[mask] = doc_labels[:np.sum(mask)]\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "        \n",
        "      #  doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "       # encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "metadata": {
        "id": "JKkNgP1ZWEyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "njb-4wk6DoR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))"
      ],
      "metadata": {
        "id": "-DyKr3QHDqlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))"
      ],
      "metadata": {
        "id": "VDexGyDKDslk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForTokenClassification"
      ],
      "metadata": {
        "id": "m2TUWyKmDuzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForTokenClassification.from_pretrained('dbmdz/bert-base-italian-cased',num_labels=len(unique_tags))\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss,metrics=metric) # can also use any keras loss fn\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BkYIYA7CDxJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset.shuffle(1000).batch(8), epochs=1,validation_data=val_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "i-MW1EoBD0IF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}